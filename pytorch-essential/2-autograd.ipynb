{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b58db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff751014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires_grad tell the torch that now it need to create a computational graph to do the back propagation\n",
    "# to optimize the value of the variable\n",
    "w = torch.rand(3, requires_grad=True)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76843a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add something to w\n",
    "# first do the forward pass\n",
    "# y has the attribute grad_fn,which points to gradient function 'AddBackward0'\n",
    "y = w+2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c526f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#diffent operation on the tensor\n",
    "# see the difference in the grad_fn\n",
    "z = y*y*2\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2231a4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now when want to calculate the gradient call the .backward() function\n",
    "#.backward() is the vector jacobian product, hence that's why in the below example we have to pass the vector\n",
    "\n",
    "#create a vector, to do the vector-jacobian multiplication (if z is not a scalar value)\n",
    "\n",
    "v = torch.tensor([0.1, 1.0, 0.001], dtype=torch.float32)\n",
    "z.backward(v)        # will cal dz/dw\n",
    "print(w.grad)        # where gradients are stored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13d3a19",
   "metadata": {},
   "source": [
    "## Stop tracking gradient history\n",
    "* how to prevent pytorch for tracking gradient history\n",
    "* option 1 :w.requires_grad_(False)\n",
    "* option2: w.detach()\n",
    "* wrap in with statement i.e with torch.no_grad():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64572b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#option1\n",
    "print(w)\n",
    "w.requires_grad_(False)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4262d644",
   "metadata": {},
   "outputs": [],
   "source": [
    "#option2- create a new tensor\n",
    "y = w.detach()\n",
    "print(w)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9c7792",
   "metadata": {},
   "outputs": [],
   "source": [
    "#option3- wrap in with statement:\n",
    "with torch.no_grad():\n",
    "    z = w+ 2\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ba8329",
   "metadata": {},
   "source": [
    "## Dummy training example\n",
    "    * key takeaway: gradients are summed up in the grad() function, hence may lead to incorrect weights update\n",
    "    * always nullify the value of grad() before using further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8d18d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    #calculating the model output\n",
    "    y = (weights*3).sum()\n",
    "    \n",
    "    #cal the grad of y w.rt to each weights using backprop\n",
    "    y.backward()\n",
    "    print(weights.grad)\n",
    "    \n",
    "    #zero the val of grad, in every training loop\n",
    "    weights.grad.zero_()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
